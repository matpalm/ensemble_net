{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data as d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 22:26:17.107448 140421431543616 ag_logging.py:146] AutoGraph could not transform <function dataset.<locals>.to_float at 0x7fb65d249620> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function dataset.<locals>.to_float at 0x7fb65d249620> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "imgs (4, 64, 64, 3)\n",
      "[[[0.14117648 0.21960784 0.3137255 ]\n",
      "  [0.13333334 0.21960784 0.30980393]\n",
      "  [0.15294118 0.23921569 0.32156864]\n",
      "  ...\n",
      "  [0.15294118 0.25490198 0.32156864]\n",
      "  [0.15686275 0.25882354 0.3254902 ]\n",
      "  [0.14901961 0.2509804  0.31764707]]\n",
      "\n",
      " [[0.13725491 0.21960784 0.3019608 ]\n",
      "  [0.13725491 0.22352941 0.30588236]\n",
      "  [0.14509805 0.24313726 0.32156864]\n",
      "  ...\n",
      "  [0.15294118 0.25490198 0.32156864]\n",
      "  [0.15294118 0.25490198 0.32156864]\n",
      "  [0.14901961 0.2509804  0.31764707]]\n",
      "\n",
      " [[0.15686275 0.24705882 0.31764707]\n",
      "  [0.14509805 0.24705882 0.3137255 ]\n",
      "  [0.15686275 0.25882354 0.3254902 ]\n",
      "  ...\n",
      "  [0.15294118 0.25490198 0.32156864]\n",
      "  [0.14901961 0.25490198 0.32156864]\n",
      "  [0.14509805 0.2509804  0.31764707]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.15686275 0.25882354 0.31764707]\n",
      "  [0.15686275 0.25882354 0.31764707]\n",
      "  [0.16470589 0.26666668 0.3254902 ]\n",
      "  ...\n",
      "  [0.16078432 0.2509804  0.3137255 ]\n",
      "  [0.17254902 0.2627451  0.3254902 ]\n",
      "  [0.16078432 0.2509804  0.3137255 ]]\n",
      "\n",
      " [[0.15686275 0.26666668 0.3137255 ]\n",
      "  [0.16078432 0.2627451  0.32156864]\n",
      "  [0.16078432 0.2627451  0.32156864]\n",
      "  ...\n",
      "  [0.15686275 0.25882354 0.31764707]\n",
      "  [0.16862746 0.25882354 0.32156864]\n",
      "  [0.15686275 0.24705882 0.3019608 ]]\n",
      "\n",
      " [[0.16862746 0.2784314  0.3254902 ]\n",
      "  [0.16862746 0.2784314  0.3254902 ]\n",
      "  [0.16862746 0.25882354 0.32941177]\n",
      "  ...\n",
      "  [0.15686275 0.25882354 0.31764707]\n",
      "  [0.16470589 0.25490198 0.31764707]\n",
      "  [0.16078432 0.2509804  0.30588236]]]\n",
      "labels [1 9 4 2]\n"
     ]
    }
   ],
   "source": [
    "ds = d.dataset('test', batch_size=4)\n",
    "for imgs, labels in ds:\n",
    "    print(\"imgs\", imgs.shape)\n",
    "    print(imgs[0])\n",
    "    print(\"labels\", labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, lax, vmap\n",
    "from jax.nn.initializers import glorot_normal, he_normal\n",
    "from jax.nn.functions import gelu\n",
    "from functools import partial\n",
    "import objax\n",
    "from objax.variable import TrainVar\n",
    "\n",
    "\n",
    "def _conv_layer(stride, activation, inp, kernel, bias):\n",
    "    no_dilation = (1, 1)\n",
    "    some_height_width = 10  # values don't matter; just shape of input\n",
    "    input_shape = (1, some_height_width, some_height_width, 3)\n",
    "    kernel_shape = (3, 3, 1, 1)\n",
    "    input_kernel_output = ('NHWC', 'HWIO', 'NHWC')\n",
    "    conv_dimension_numbers = lax.conv_dimension_numbers(input_shape,\n",
    "                                                        kernel_shape,\n",
    "                                                        input_kernel_output)\n",
    "    block = lax.conv_general_dilated(inp, kernel, (stride, stride),\n",
    "                                     'VALID', no_dilation, no_dilation,\n",
    "                                     conv_dimension_numbers)\n",
    "    if bias is not None:\n",
    "        block += bias\n",
    "    if activation:\n",
    "        block = activation(block)\n",
    "    return block\n",
    "\n",
    "def _dense_layer(activation, inp, kernel, bias):\n",
    "    block = jnp.dot(inp, kernel) + bias\n",
    "    if activation:\n",
    "        block = activation(block)\n",
    "    return block\n",
    "\n",
    "# def _conv_block_without_bias(stride, with_non_linearity, inp, kernel):\n",
    "#     # the need for this method feels a bit clunky :/ is there a better\n",
    "#     # way to vmap with the None?\n",
    "#     return _conv_block(stride, with_non_linearity, inp, kernel, None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonEnsembleNet(objax.Module):\n",
    "\n",
    "    def __init__(self, num_classes, dense_kernel_size=32, seed=0):\n",
    "\n",
    "        key = random.PRNGKey(seed)\n",
    "        subkeys = random.split(key, 8)\n",
    "\n",
    "        # conv stack kernels and biases\n",
    "        self.conv_kernels = objax.ModuleList()\n",
    "        self.conv_biases = objax.ModuleList()\n",
    "        input_channels = 3\n",
    "        for i, output_channels in enumerate([32, 64, 64, 64]):\n",
    "            self.conv_kernels.append(TrainVar(he_normal()(subkeys[i], (3, 3, input_channels,\n",
    "                                                                       output_channels))))\n",
    "            self.conv_biases.append(TrainVar(jnp.zeros((output_channels))))\n",
    "            input_channels = output_channels\n",
    "\n",
    "        # dense layer kernel and bias\n",
    "        self.dense_kernel = TrainVar(he_normal()(subkeys[6], (output_channels, dense_kernel_size)))\n",
    "        self.dense_bias = TrainVar(jnp.zeros((dense_kernel_size)))\n",
    "\n",
    "        # classifier layer kernel and bias\n",
    "        self.logits_kernel = TrainVar(glorot_normal()(subkeys[6], (dense_kernel_size, num_classes)))\n",
    "        self.logits_bias = TrainVar(jnp.zeros((num_classes)))\n",
    "\n",
    "    def logits(self, inp):        \n",
    "        # conv stack -> (B, 3, 3, 64)\n",
    "        y = inp\n",
    "        for kernel, bias in zip(self.conv_kernels, self.conv_biases):\n",
    "            y = _conv_layer(2, gelu, y, kernel.value, bias.value)\n",
    "            \n",
    "        # global spatial pooling -> (B, 64)\n",
    "        y = jnp.mean(y, axis=(1, 2))\n",
    "            \n",
    "        # dense layer with non linearity -> (B, 32)\n",
    "        y = _dense_layer(gelu, y, self.dense_kernel.value, self.dense_bias.value)\n",
    "        \n",
    "        # dense layer with no activation to number classes -> (B, num_classes)\n",
    "        logits = _dense_layer(None, y, self.logits_kernel.value, self.logits_bias.value)\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "    def predict(self, inp):\n",
    "        return jax.nn.softmax(self.logits(inp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleNet(objax.Module):\n",
    "\n",
    "    def __init__(self, num_models, num_classes, dense_kernel_size=32, seed=0):\n",
    "\n",
    "        key = random.PRNGKey(seed)\n",
    "        subkeys = random.split(key, 8)\n",
    "\n",
    "        # conv stack kernels and biases\n",
    "        self.conv_kernels = objax.ModuleList()\n",
    "        self.conv_biases = objax.ModuleList()\n",
    "        input_channels = 3\n",
    "        for i, output_channels in enumerate([32, 64, 64, 64]):\n",
    "            self.conv_kernels.append(TrainVar(he_normal()(subkeys[i], (num_models, 3, 3, input_channels,\n",
    "                                                                       output_channels))))\n",
    "            self.conv_biases.append(TrainVar(jnp.zeros((num_models, output_channels))))\n",
    "            input_channels = output_channels\n",
    "\n",
    "        # dense layer kernel and bias\n",
    "        self.dense_kernels = TrainVar(he_normal()(subkeys[6], \n",
    "                                                 (num_models, output_channels, dense_kernel_size)))\n",
    "        self.dense_biases = TrainVar(jnp.zeros((num_models, dense_kernel_size)))\n",
    "\n",
    "        # classifier layer kernel and bias\n",
    "        self.logits_kernel = TrainVar(glorot_normal()(subkeys[6], \n",
    "                                                      (num_models, dense_kernel_size, num_classes)))\n",
    "        self.logits_biases = TrainVar(jnp.zeros((num_models, num_classes)))\n",
    "\n",
    "    def logits(self, inp):                \n",
    "        # the first call vmaps over the first conv params for a single input\n",
    "        y = vmap(partial(_conv_layer, 2, gelu, inp))(\n",
    "            self.conv_kernels[0].value, self.conv_biases[0].value)        \n",
    "\n",
    "        # subsequent calls vmap over both the prior input and the conv params\n",
    "        # the first representing the batched input with the second representing\n",
    "        # the batched models (i.e. the ensemble)\n",
    "        for conv_kernel, conv_bias in zip(self.conv_kernels[1:],\n",
    "                                          self.conv_biases[1:]):\n",
    "            y = vmap(partial(_conv_layer, 2, gelu))(\n",
    "                y, conv_kernel.value, conv_bias.value)\n",
    "            \n",
    "    \n",
    "        # global spatial pooling\n",
    "        # (M, B, 64)\n",
    "        y = jnp.mean(y, axis=(2, 3))\n",
    "\n",
    "        # dense layer with non linearity\n",
    "        # (M, B, 32)\n",
    "        y = vmap(partial(_dense_layer, gelu))(y, self.dense_kernels.value, self.dense_biases.value)\n",
    "        \n",
    "        # dense layer with no activation to number classes \n",
    "        # (M, B, num_classes)\n",
    "        logits = vmap(partial(_dense_layer, None))(y, self.logits_kernel.value, self.logits_biases.value)        \n",
    "        return logits\n",
    "        \n",
    "    def predict(self, inp):\n",
    "        return jax.nn.softmax(self.logits(inp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 10)\n",
      "[[[0.10004424 0.09997072 0.10037523 0.09999789 0.09989534 0.09969421\n",
      "   0.0997958  0.0999829  0.09994222 0.10030156]\n",
      "  [0.10007226 0.09998271 0.10032347 0.09999003 0.09988236 0.09975057\n",
      "   0.09981922 0.09997445 0.09994675 0.10025822]\n",
      "  [0.10015408 0.10009208 0.10018627 0.09996849 0.10000385 0.09928628\n",
      "   0.09951436 0.10046441 0.09990092 0.10042922]\n",
      "  [0.09998829 0.09993898 0.10031915 0.09999322 0.09994603 0.09977533\n",
      "   0.09986364 0.09995876 0.09999613 0.10022039]]\n",
      "\n",
      " [[0.10038323 0.09982816 0.09969807 0.10003973 0.10005305 0.10000939\n",
      "   0.09987422 0.10008911 0.09997763 0.10004737]\n",
      "  [0.10029979 0.09988333 0.09974554 0.10003419 0.10004012 0.09995164\n",
      "   0.09993222 0.10008802 0.09998222 0.10004304]\n",
      "  [0.10099845 0.09958512 0.09945692 0.09989917 0.10006463 0.10026194\n",
      "   0.09961688 0.1000125  0.10013781 0.09996644]\n",
      "  [0.1003595  0.09983999 0.09973717 0.1000357  0.10005657 0.10006701\n",
      "   0.09982106 0.1000558  0.09995658 0.10007066]]\n",
      "\n",
      " [[0.0995431  0.10018515 0.10007327 0.10008618 0.09997998 0.09985171\n",
      "   0.10002135 0.1000637  0.10038291 0.0998126 ]\n",
      "  [0.09966367 0.10015014 0.10010845 0.10002374 0.09999302 0.09988105\n",
      "   0.10001339 0.1000543  0.10023923 0.09987304]\n",
      "  [0.09896892 0.10020168 0.09946617 0.10065999 0.09973272 0.10001913\n",
      "   0.10002521 0.09989135 0.10153152 0.09950328]\n",
      "  [0.09956726 0.10018796 0.1000292  0.1000886  0.09999762 0.09986174\n",
      "   0.10001245 0.10006098 0.10038598 0.09980818]]]\n"
     ]
    }
   ],
   "source": [
    "net = EnsembleNet(num_models=3, num_classes=10)\n",
    "print(net.logits(imgs).shape)\n",
    "print(net.predict(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.074      -0.045      -0.11100001  0.053       0.054      -0.001\n",
      "  -0.109       0.047      -0.004       0.064     ]\n",
      " [-0.059      -0.036      -0.083       0.037       0.043       0.005\n",
      "  -0.089       0.038       0.006       0.051     ]\n",
      " [-0.42200002 -0.28800002 -0.252       0.27400002  0.19700001 -0.22000001\n",
      "  -0.45200002  0.216       0.1         0.115     ]\n",
      " [-0.063      -0.039      -0.10300001  0.048       0.049      -0.003\n",
      "  -0.10200001  0.041      -0.011       0.06500001]]\n",
      "[[0.094      0.097      0.09       0.10700001 0.10700001 0.101\n",
      "  0.09100001 0.10600001 0.101      0.108     ]\n",
      " [0.09500001 0.097      0.093      0.105      0.105      0.101\n",
      "  0.09200001 0.105      0.101      0.10600001]\n",
      " [0.068      0.078      0.081      0.13700001 0.127      0.083\n",
      "  0.066      0.12900001 0.115      0.11700001]\n",
      " [0.09500001 0.097      0.09100001 0.10600001 0.10600001 0.101\n",
      "  0.09100001 0.105      0.1        0.108     ]]\n"
     ]
    }
   ],
   "source": [
    "net = NonEnsembleNet(num_classes=10)\n",
    "print(jnp.around(net.logits(imgs), 3))\n",
    "print(jnp.around(net.predict(imgs), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 64, 64, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(imgs, labels):\n",
    "    logits = net.logits(imgs)\n",
    "    return jnp.mean(objax.functional.loss.cross_entropy_logits_sparse(logits, labels))\n",
    "\n",
    "gradient_loss = objax.GradValues(cross_entropy, net.vars())\n",
    "optimiser = objax.optimizer.Adam(net.vars())\n",
    "lr = 1e-3\n",
    "\n",
    "# create a jitted training step\n",
    "def train_step(imgs, labels):\n",
    "    grads, loss = gradient_loss(imgs, labels)\n",
    "    optimiser(lr, grads)\n",
    "    return loss\n",
    "\n",
    "train_step = objax.Jit(train_step,\n",
    "                       gradient_loss.vars() + optimiser.vars())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeviceArray(1.2861435, dtype=float32)]\n",
      "[DeviceArray(1.175744, dtype=float32)]\n",
      "[DeviceArray(1.0832641, dtype=float32)]\n",
      "[DeviceArray(1.0269818, dtype=float32)]\n",
      "[DeviceArray(0.9848052, dtype=float32)]\n",
      "[DeviceArray(0.93037367, dtype=float32)]\n",
      "[DeviceArray(0.8638202, dtype=float32)]\n",
      "[DeviceArray(0.8097404, dtype=float32)]\n",
      "[DeviceArray(0.77046627, dtype=float32)]\n",
      "[DeviceArray(0.70041114, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(train_step(imgs, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.        , 0.38      , 0.        , 0.17      , 0.01      ,\n",
       "              0.        , 0.        , 0.02      , 0.01      , 0.41      ],\n",
       "             [0.        , 0.35999998, 0.        , 0.31      , 0.        ,\n",
       "              0.        , 0.        , 0.01      , 0.02      , 0.29      ],\n",
       "             [0.        , 0.        , 0.        , 0.02      , 0.        ,\n",
       "              0.        , 0.        , 0.        , 0.97999996, 0.        ],\n",
       "             [0.        , 0.22      , 0.        , 0.53      , 0.        ,\n",
       "              0.        , 0.        , 0.        , 0.05      , 0.19      ]],            dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.around(net.predict(imgs), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 1, 8, 3])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
